# LLM Integration Specification

spec_id: llm-integration-v1
component: AnthropicLLMClient
location: src/agentforge/core/llm/
test_location: tests/unit/llm/test_client.py

## Purpose

Native Anthropic API integration with:
- Native tool use (not text-based parsing)
- Extended thinking for complex tasks
- Prompt caching optimization
- Streaming support

## Key Insight

**We have FULL API parity with Claude Code.** Native tools and extended thinking are available via the API. Previous implementation used text-based tool parsing - this should be migrated to native tools.

## Components

### AnthropicLLMClient

```python
class AnthropicLLMClient:
    DEFAULT_MODEL = "claude-sonnet-4-20250514"
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = DEFAULT_MODEL,
        max_tokens: int = 4096,
    ):
        self.client = Anthropic(api_key=api_key)
        self.model = model
        self.max_tokens = max_tokens
    
    def complete(
        self,
        system: str,
        messages: List[Dict],
        tools: Optional[List[ToolDefinition]] = None,
        thinking: Optional[ThinkingConfig] = None,
        tool_choice: str = "auto",
    ) -> LLMResponse:
        """
        Make completion with native tools and thinking.
        
        Args:
            system: System prompt (keep small for caching)
            messages: Conversation messages
            tools: Native tool definitions
            thinking: Extended thinking config
            tool_choice: "auto"|"any"|"tool"
        """
    
    def complete_with_tools(
        self,
        system: str,
        messages: List[Dict],
        tools: List[ToolDefinition],
        tool_executor: Callable,
        max_iterations: int = 10,
        thinking: Optional[ThinkingConfig] = None,
    ) -> LLMResponse:
        """
        Complete with automatic tool execution loop.
        Continues until text response or max iterations.
        """
    
    def stream(
        self,
        system: str,
        messages: List[Dict],
        tools: Optional[List[ToolDefinition]] = None,
    ) -> Iterator[str]:
        """Stream completion response."""
```

### Native Tool Definitions

```python
@dataclass
class ToolDefinition:
    name: str
    description: str
    input_schema: Dict[str, Any]  # JSON Schema

# Example
ToolDefinition(
    name="extract_function",
    description="Extract code into a new helper function",
    input_schema={
        "type": "object",
        "properties": {
            "file_path": {"type": "string"},
            "source_function": {"type": "string"},
            "start_line": {"type": "integer"},
            "end_line": {"type": "integer"},
            "new_function_name": {"type": "string"},
        },
        "required": ["file_path", "source_function", "start_line", "end_line", "new_function_name"]
    }
)
```

### Extended Thinking

```python
class ThinkingConfig(BaseModel):
    enabled: bool = False
    budget_tokens: int = 8000

# Usage
response = client.complete(
    system=system,
    messages=messages,
    thinking=ThinkingConfig(enabled=True, budget_tokens=10000),
)

# Response includes thinking
for block in response.content:
    if block.type == "thinking":
        print(f"Thinking: {block.thinking}")  # Full transparency!
```

## API Request Structure

```python
request = {
    "model": "claude-sonnet-4-20250514",
    "max_tokens": 4096,
    "system": system_prompt,  # Keep small for caching
    "messages": messages,
    
    # Native tools
    "tools": [
        {
            "name": "read_file",
            "description": "Read file contents",
            "input_schema": {...}
        }
    ],
    "tool_choice": {"type": "auto"},
    
    # Extended thinking (optional)
    "thinking": {
        "type": "enabled",
        "budget_tokens": 8000
    }
}
```

## Response Structure

```python
@dataclass
class LLMResponse:
    content: str              # Text content
    tool_calls: List[Dict]    # Tool use blocks
    thinking: Optional[str]   # Thinking content (transparency!)
    usage: Dict[str, int]     # Token usage
    stop_reason: str

# Tool call format
{
    "id": "toolu_xxx",
    "name": "extract_function",
    "input": {
        "file_path": "/src/file.py",
        "start_line": 42,
        ...
    }
}
```

## Tool Categories

### Base Tools (all task types)

| Tool | Description |
|------|-------------|
| read_file | Read file contents |
| write_file | Write content to file |
| search_code | Search for patterns |
| run_tests | Execute tests |
| complete | Mark task complete |
| escalate | Request human help |

### Refactoring Tools (fix_violation, refactor)

| Tool | Description |
|------|-------------|
| extract_function | Extract code to helper |
| simplify_conditional | Convert to guard clause |
| run_check | Run conformance check |
| inline_function | Inline a function |
| rename_symbol | Rename with references |

### Discovery Tools (discovery, bridge)

| Tool | Description |
|------|-------------|
| analyze_dependencies | Map import graph |
| detect_patterns | Find code patterns |
| map_structure | Analyze project structure |

## Prompt Caching

System prompts are automatically cached by Anthropic:
- First call: Cache miss, full price
- Subsequent calls: Cache hit, 90% discount

**Strategy**: Keep system prompt small (~150 tokens) and stable.
Put task-specific content in user message.

## Usage Tracking

```python
def get_usage_stats(self) -> Dict[str, int]:
    return {
        "total_input_tokens": self._total_input_tokens,
        "total_output_tokens": self._total_output_tokens,
        "cached_tokens": self._cached_tokens,
        "effective_input_tokens": self._total_input_tokens - int(self._cached_tokens * 0.9),
    }
```

## Migration from Text-Based Parsing

### Before (text-based)

```python
# Old approach - parse from response text
response_text = call_llm(prompt)
action_match = re.search(r"```action\n(.*?)```", response_text)
action_data = yaml.safe_load(action_match.group(1))
```

### After (native tools)

```python
# New approach - native tool use
response = client.complete(
    system=system,
    messages=messages,
    tools=tools,
)

for tool_call in response.tool_calls:
    result = execute_tool(tool_call["name"], tool_call["input"])
```

## Test Cases

```yaml
tests:
  - name: test_native_tool_call
    setup: Request with tools, model returns tool_use
    assert: tool_calls populated correctly
    
  - name: test_thinking_captured
    setup: Request with thinking enabled
    assert: response.thinking contains reasoning
    
  - name: test_tool_loop_executes
    setup: Tool that requires follow-up
    assert: Loop continues until text response
    
  - name: test_usage_tracking
    setup: Multiple requests
    assert: Cumulative usage tracked
    
  - name: test_streaming
    setup: Stream request
    assert: Yields text chunks
```

## Tool Registry

### get_tools_for_task()

```python
from agentforge.core.llm.tools import get_tools_for_task

# Get tools for a task type
tools = get_tools_for_task("fix_violation")
# Returns: [READ_FILE, WRITE_FILE, ..., EXTRACT_FUNCTION, RUN_CHECK, ...]

# Use with LLM client
response = client.complete(
    system=system,
    messages=messages,
    tools=tools,
)
```

### Tool Categories by Task

| Task Type | Base Tools | Additional Tools |
|-----------|------------|------------------|
| fix_violation | read, write, edit, search, complete, escalate | extract_function, simplify_conditional, run_check, inline_function, rename_symbol |
| implement_feature | read, write, edit, search, complete, escalate | generate_test, run_single_test |
| write_tests | read, write, edit, search, complete, escalate | generate_test, run_single_test |
| refactor | read, write, edit, search, complete, escalate | extract_function, simplify_conditional, run_check, inline_function, rename_symbol |
| discovery | read, search, complete, escalate | analyze_dependencies, detect_patterns, map_structure |
| bridge | read, search, complete, escalate | analyze_dependencies, detect_patterns, map_structure, create_mapping |
| code_review | read, search, complete, escalate | analyze_diff, add_review_comment, generate_review_summary |

## Components

| Component | File | Test |
|-----------|------|------|
| llm-interface | interface.py | test_interface.py |
| llm-client | client.py | test_client.py |
| llm-simulated | simulated.py | test_simulated.py |
| llm-factory | factory.py | test_factory.py |
| llm-recording | recording.py | test_recording.py |
| llm-tools | tools.py | test_tools.py |

## Links

- Implementation: `src/agentforge/core/llm/`
- Tests: `tests/unit/llm/`
- Anthropic Docs: https://docs.anthropic.com/en/docs/build-with-claude/tool-use
- Extended Thinking: https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking
