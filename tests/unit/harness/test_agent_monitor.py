# @spec_file: .agentforge/specs/core-harness-v1.yaml
# @spec_id: core-harness-v1
# @component_id: tools-harness-agent_monitor
# @impl_path: tools/harness/agent_monitor.py

# Generated by AgentForge
# Spec: agent_monitor
# Phase: red
# Date: 2025-12-31 05:34:01 UTC

"""Tests for AgentMonitor component."""

from agentforge.core.harness.agent_monitor import AgentMonitor, MonitorConfig
from agentforge.core.harness.monitor_domain import (
    AgentHealth,
    HealthStatus,
    ObservationType,
    Recommendation,
)


class TestAgentMonitorInit:
    """Test AgentMonitor initialization."""

    def test_init_with_default_config(self):
        """Test initialization with default config."""
        monitor = AgentMonitor()

        # Should have default config values
        assert monitor.config.history_window == 100, "Expected monitor.config.history_window to equal 100"
        assert monitor.config.loop_threshold == 3, "Expected monitor.config.loop_threshold to equal 3"
        assert monitor.config.drift_threshold == 0.5, "Expected monitor.config.drift_threshold to equal 0.5"
        assert monitor.config.thrash_threshold == 3, "Expected monitor.config.thrash_thres... to equal 3"
        assert monitor.config.stall_threshold == 5, "Expected monitor.config.stall_threshold to equal 5"

    def test_init_with_custom_config(self):
        """Test initialization with custom config."""
        config = MonitorConfig(
            history_window=50,
            loop_threshold=2,
            drift_threshold=0.3,
            thrash_threshold=2,
            stall_threshold=3
        )

        monitor = AgentMonitor(config=config)

        assert monitor.config.history_window == 50, "Expected monitor.config.history_window to equal 50"
        assert monitor.config.loop_threshold == 2, "Expected monitor.config.loop_threshold to equal 2"
        assert monitor.config.drift_threshold == 0.3, "Expected monitor.config.drift_threshold to equal 0.3"
        assert monitor.config.thrash_threshold == 2, "Expected monitor.config.thrash_thres... to equal 2"
        assert monitor.config.stall_threshold == 3, "Expected monitor.config.stall_threshold to equal 3"


class TestAgentMonitorObservations:
    """Test observation recording methods."""

    def test_observe_action_basic(self):
        """Test observing a basic action."""
        monitor = AgentMonitor()

        monitor.observe_action("create_file", {"filename": "test.py"})

        observations = monitor.get_recent_observations(count=1)
        assert len(observations) == 1, "Expected len(observations) to equal 1"
        assert observations[0].type == ObservationType.ACTION, "Expected observations[0].type to equal ObservationType.ACTION"
        assert observations[0].data["action"] == "create_file", "Expected observations[0].data['action'] to equal 'create_file'"
        assert observations[0].data["details"]["filename"] == "test.py", "Expected observations[0].data['detai... to equal 'test.py'"

    def test_observe_action_without_details(self):
        """Test observing action without details."""
        monitor = AgentMonitor()

        monitor.observe_action("simple_action")

        observations = monitor.get_recent_observations(count=1)
        assert len(observations) == 1, "Expected len(observations) to equal 1"
        assert observations[0].data["action"] == "simple_action", "Expected observations[0].data['action'] to equal 'simple_action'"
        assert observations[0].data.get("details") is None, "Expected observations[0].data.get('d... is None"

    def test_observe_output(self):
        """Test observing agent output."""
        monitor = AgentMonitor()

        monitor.observe_output("print('hello')", "console")

        observations = monitor.get_recent_observations(count=1)
        assert len(observations) == 1, "Expected len(observations) to equal 1"
        assert observations[0].type == ObservationType.OUTPUT, "Expected observations[0].type to equal ObservationType.OUTPUT"
        assert observations[0].data["output"] == "print('hello')", "Expected observations[0].data['output'] to equal \"print('hello')\""
        assert observations[0].data["output_type"] == "console", "Expected observations[0].data['outpu... to equal 'console'"

    def test_observe_verification_passed(self):
        """Test observing successful verification."""
        monitor = AgentMonitor()

        monitor.observe_verification(True, {"test": "unit_test_1"})

        observations = monitor.get_recent_observations(count=1)
        assert len(observations) == 1, "Expected len(observations) to equal 1"
        assert observations[0].type == ObservationType.VERIFICATION, "Expected observations[0].type to equal ObservationType.VERIFICATION"
        assert observations[0].data["passed"] is True, "Expected observations[0].data['passed'] is True"
        assert observations[0].data["details"]["test"] == "unit_test_1", "Expected observations[0].data['detai... to equal 'unit_test_1'"

    def test_observe_verification_failed(self):
        """Test observing failed verification."""
        monitor = AgentMonitor()

        monitor.observe_verification(False)

        observations = monitor.get_recent_observations(count=1)
        assert len(observations) == 1, "Expected len(observations) to equal 1"
        assert observations[0].data["passed"] is False, "Expected observations[0].data['passed'] is False"
        assert observations[0].data.get("details") is None, "Expected observations[0].data.get('d... is None"

    def test_observe_state_change(self):
        """Test observing state change."""
        monitor = AgentMonitor()

        monitor.observe_state_change("planning", "implementation")

        observations = monitor.get_recent_observations(count=1)
        assert len(observations) == 1, "Expected len(observations) to equal 1"
        assert observations[0].type == ObservationType.STATE_CHANGE, "Expected observations[0].type to equal ObservationType.STATE_CHANGE"
        assert observations[0].data["old_state"] == "planning", "Expected observations[0].data['old_s... to equal 'planning'"
        assert observations[0].data["new_state"] == "implementation", "Expected observations[0].data['new_s... to equal 'implementation'"

    def test_observe_error(self):
        """Test observing error."""
        monitor = AgentMonitor()

        monitor.observe_error("SyntaxError", "Invalid syntax", {"line": 42})

        observations = monitor.get_recent_observations(count=1)
        assert len(observations) == 1, "Expected len(observations) to equal 1"
        assert observations[0].type == ObservationType.ERROR, "Expected observations[0].type to equal ObservationType.ERROR"
        assert observations[0].data["error_type"] == "SyntaxError", "Expected observations[0].data['error... to equal 'SyntaxError'"
        assert observations[0].data["message"] == "Invalid syntax", "Expected observations[0].data['messa... to equal 'Invalid syntax'"
        assert observations[0].data["details"]["line"] == 42, "Expected observations[0].data['detai... to equal 42"


class TestAgentMonitorLoopDetection:
    """Test loop detection functionality."""

    def test_detect_loop_no_loop(self):
        """Test loop detection when no loop exists."""
        monitor = AgentMonitor()

        # Add different actions
        monitor.observe_action("action1")
        monitor.observe_action("action2")
        monitor.observe_action("action3")

        result = monitor.detect_loop()

        assert result is None or result.detected is False, "Assertion failed"

    def test_detect_loop_action_repetition(self):
        """Test loop detection for repeated actions."""
        monitor = AgentMonitor()

        # Add same action multiple times (above threshold)
        for _ in range(4):  # Above default threshold of 3
            monitor.observe_action("repeated_action")

        result = monitor.detect_loop()

        assert result is not None, "Expected result is not None"
        assert result.detected is True, "Expected result.detected is True"
        assert result.count >= 3, "Expected result.count >= 3"
        assert "repeated_action" in result.pattern, "Expected 'repeated_action' in result.pattern"

    def test_detect_loop_error_repetition(self):
        """Test loop detection for repeated errors."""
        monitor = AgentMonitor()

        # Add same error multiple times
        for _ in range(4):
            monitor.observe_error("FileNotFoundError", "File missing")

        result = monitor.detect_loop()

        assert result is not None, "Expected result is not None"
        assert result.detected is True, "Expected result.detected is True"
        assert result.count >= 3, "Expected result.count >= 3"

    def test_detect_loop_state_cycle(self):
        """Test loop detection for state cycles."""
        monitor = AgentMonitor()

        # Create A->B->A->B pattern
        monitor.observe_state_change("idle", "active")
        monitor.observe_state_change("active", "idle")
        monitor.observe_state_change("idle", "active")
        monitor.observe_state_change("active", "idle")

        result = monitor.detect_loop()

        assert result is not None, "Expected result is not None"
        assert result.detected is True, "Expected result.detected is True"


class TestAgentMonitorDriftDetection:
    """Test drift detection functionality."""

    def test_detect_drift_no_drift(self):
        """Test drift detection when actions align with task."""
        monitor = AgentMonitor()
        original_task = "create a Python file with unit tests"

        # Add relevant actions that match task keywords
        monitor.observe_action("create_python_file", {"filename": "test.py"})
        monitor.observe_action("write_unit_tests", {"type": "unit_test"})
        monitor.observe_action("run_tests")

        drift_score = monitor.detect_drift(original_task)

        assert 0.0 <= drift_score <= 1.0, "Assertion failed"
        assert drift_score < 0.3, "Expected drift_score < 0.3"# Should be low drift with matching keywords

    def test_detect_drift_high_drift(self):
        """Test drift detection when actions diverge from task."""
        monitor = AgentMonitor()
        original_task = "create a Python file with unit tests"

        # Add unrelated actions
        monitor.observe_action("browse_web", {"url": "example.com"})
        monitor.observe_action("send_email", {"to": "user@example.com"})

        drift_score = monitor.detect_drift(original_task)

        assert 0.0 <= drift_score <= 1.0, "Assertion failed"
        assert drift_score > 0.5, "Expected drift_score > 0.5"# Should be high drift

    def test_detect_drift_empty_history(self):
        """Test drift detection with no observations."""
        monitor = AgentMonitor()
        original_task = "create a Python file"

        drift_score = monitor.detect_drift(original_task)

        assert drift_score == 0.0, "Expected drift_score to equal 0.0"# No observations means no drift


class TestAgentMonitorThrashingDetection:
    """Test thrashing detection functionality."""

    def test_detect_thrashing_no_thrashing(self):
        """Test thrashing detection when no thrashing occurs."""
        monitor = AgentMonitor()

        # Add different file modifications
        monitor.observe_action("modify_file", {"file": "file1.py"})
        monitor.observe_action("modify_file", {"file": "file2.py"})
        monitor.observe_action("modify_file", {"file": "file3.py"})

        result = monitor.detect_thrashing()

        assert result is None or result.detected is False, "Assertion failed"

    def test_detect_thrashing_file_modifications(self):
        """Test thrashing detection for repeated file modifications."""
        monitor = AgentMonitor()

        # Modify same file multiple times (above threshold)
        for _ in range(4):  # Above default threshold of 3
            monitor.observe_action("modify_file", {"file": "thrashed.py"})

        result = monitor.detect_thrashing()

        assert result is not None, "Expected result is not None"
        assert result.detected is True, "Expected result.detected is True"
        assert "thrashed.py" in result.affected_files, "Expected 'thrashed.py' in result.affected_files"
        assert result.alternation_count >= 3, "Expected result.alternation_count >= 3"

    def test_detect_thrashing_alternating_states(self):
        """Test thrashing detection for alternating states."""
        monitor = AgentMonitor()

        # Create alternating pattern
        for _ in range(2):
            monitor.observe_state_change("state_a", "state_b")
            monitor.observe_state_change("state_b", "state_a")

        result = monitor.detect_thrashing()

        assert result is not None, "Expected result is not None"
        assert result.detected is True, "Expected result.detected is True"


class TestAgentMonitorContextAndProgress:
    """Test context pressure and progress scoring."""

    def test_get_context_pressure_low(self):
        """Test context pressure calculation for low usage."""
        monitor = AgentMonitor()

        pressure = monitor.get_context_pressure(tokens_used=1000, token_budget=10000)

        assert pressure == 0.1, "Expected pressure to equal 0.1"
        assert 0.0 <= pressure <= 1.0, "Assertion failed"

    def test_get_context_pressure_high(self):
        """Test context pressure calculation for high usage."""
        monitor = AgentMonitor()

        pressure = monitor.get_context_pressure(tokens_used=9500, token_budget=10000)

        assert pressure == 0.95, "Expected pressure to equal 0.95"
        assert 0.0 <= pressure <= 1.0, "Assertion failed"

    def test_get_context_pressure_over_budget(self):
        """Test context pressure when over budget."""
        monitor = AgentMonitor()

        pressure = monitor.get_context_pressure(tokens_used=12000, token_budget=10000)

        assert pressure >= 1.0, "Expected pressure >= 1.0"

    def test_get_progress_score_high_progress(self):
        """Test progress scoring with successful verifications."""
        monitor = AgentMonitor()

        # Add successful verifications
        for _ in range(3):
            monitor.observe_verification(True)
        monitor.observe_verification(False)  # One failure

        progress = monitor.get_progress_score()

        assert 0.0 <= progress <= 1.0, "Assertion failed"
        assert progress > 0.5, "Expected progress > 0.5"# Should be high progress (3/4 = 0.75)

    def test_get_progress_score_low_progress(self):
        """Test progress scoring with failed verifications."""
        monitor = AgentMonitor()

        # Add mostly failed verifications
        monitor.observe_verification(True)   # One success
        for _ in range(3):
            monitor.observe_verification(False)  # Three failures

        progress = monitor.get_progress_score()

        assert 0.0 <= progress <= 1.0, "Assertion failed"
        assert progress < 0.5, "Expected progress < 0.5"# Should be low progress (1/4 = 0.25)

    def test_get_progress_score_no_verifications(self):
        """Test progress scoring with no verifications."""
        monitor = AgentMonitor()

        progress = monitor.get_progress_score()

        assert progress == 0.5, "Expected progress to equal 0.5"# No verifications means neutral progress


class TestAgentMonitorHealthAssessment:
    """Test complete health assessment."""

    def test_get_health_healthy_agent(self):
        """Test health assessment for healthy agent."""
        monitor = AgentMonitor()

        # Add healthy observations
        monitor.observe_action("create_file")
        monitor.observe_verification(True)

        health = monitor.get_health(
            original_task="create a file",
            tokens_used=1000,
            token_budget=10000
        )

        assert isinstance(health, AgentHealth), "Expected isinstance() to be truthy"
        assert health.status == HealthStatus.HEALTHY, "Expected health.status to equal HealthStatus.HEALTHY"
        assert health.recommendation == Recommendation.CONTINUE, "Expected health.recommendation to equal Recommendation.CONTINUE"
        assert len(health.issues) == 0, "Expected len(health.issues) to equal 0"
        assert health.drift_score < 0.3, "Expected health.drift_score < 0.3"
        assert health.context_pressure < 0.8, "Expected health.context_pressure < 0.8"
        assert health.progress_score >= 0.0, "Expected health.progress_score >= 0.0"

    def test_get_health_degraded_agent(self):
        """Test health assessment for degraded agent."""
        monitor = AgentMonitor()

        # Add some concerning patterns but not critical
        for _ in range(2):  # Below critical threshold
            monitor.observe_action("repeated_action")

        health = monitor.get_health(
            tokens_used=8500,  # High but not critical context pressure
            token_budget=10000
        )

        assert health.status in [HealthStatus.HEALTHY, HealthStatus.DEGRADED], "Expected health.status in [HealthStatus.HEALTHY, Heal..."
        if health.status == HealthStatus.DEGRADED:
            assert health.recommendation == Recommendation.CHECKPOINT, "Expected health.recommendation to equal Recommendation.CHECKPOINT"

    def test_get_health_critical_agent(self):
        """Test health assessment for critical agent."""
        monitor = AgentMonitor()

        # Add critical patterns
        for _ in range(6):  # Above critical threshold
            monitor.observe_action("repeated_action")

        health = monitor.get_health(
            tokens_used=9800,  # Very high context pressure
            token_budget=10000
        )

        assert health.status == HealthStatus.CRITICAL, "Expected health.status to equal HealthStatus.CRITICAL"
        assert health.recommendation in [Recommendation.ESCALATE, Recommendation.ABORT], "Expected health.recommendation in [Recommendation.ESCALATE, R..."
        assert len(health.issues) > 0, "Expected len(health.issues) > 0"

    def test_get_health_with_loop_detection(self):
        """Test health assessment includes loop detection."""
        monitor = AgentMonitor()

        # Create loop condition
        for _ in range(4):
            monitor.observe_action("loop_action")

        health = monitor.get_health()

        if health.loop_detection:
            assert health.loop_detection.detected is True, "Expected health.loop_detection.detected is True"
            assert health.loop_detection.count >= 3, "Expected health.loop_detection.count >= 3"

    def test_get_health_with_thrashing_detection(self):
        """Test health assessment includes thrashing detection."""
        monitor = AgentMonitor()

        # Create thrashing condition
        for _ in range(4):
            monitor.observe_action("modify_file", {"file": "same_file.py"})

        health = monitor.get_health()

        if health.thrashing_detection:
            assert health.thrashing_detection.detected is True, "Expected health.thrashing_detection.... is True"


class TestAgentMonitorHistoryManagement:
    """Test history management functionality."""

    def test_clear_history(self):
        """Test clearing observation history."""
        monitor = AgentMonitor()

        # Add some observations
        monitor.observe_action("action1")
        monitor.observe_action("action2")

        assert len(monitor.get_recent_observations()) == 2, "Expected len(monitor.get_recent_obse... to equal 2"

        monitor.clear_history()

        assert len(monitor.get_recent_observations()) == 0, "Expected len(monitor.get_recent_obse... to equal 0"

    def test_get_recent_observations_count_limit(self):
        """Test getting recent observations with count limit."""
        monitor = AgentMonitor()

        # Add more observations than requested
        for i in range(5):
            monitor.observe_action(f"action_{i}")

        recent = monitor.get_recent_observations(count=3)

        assert len(recent) == 3, "Expected len(recent) to equal 3"
        # Should get most recent observations
        assert recent[0].data["action"] == "action_4", "Expected recent[0].data['action'] to equal 'action_4'"# Most recent first

    def test_get_recent_observations_type_filter(self):
        """Test getting recent observations with type filter."""
        monitor = AgentMonitor()

        # Add mixed observation types
        monitor.observe_action("test_action")
        monitor.observe_error("TestError", "test message")
        monitor.observe_verification(True)

        # Filter for only actions
        actions = monitor.get_recent_observations(
            count=10,
            type_filter=ObservationType.ACTION
        )

        assert len(actions) == 1, "Expected len(actions) to equal 1"
        assert actions[0].type == ObservationType.ACTION, "Expected actions[0].type to equal ObservationType.ACTION"
        assert actions[0].data["action"] == "test_action", "Expected actions[0].data['action'] to equal 'test_action'"

    def test_history_window_bounded(self):
        """Test that history respects window size limit."""
        config = MonitorConfig(history_window=3)
        monitor = AgentMonitor(config=config)

        # Add more observations than window size
        for i in range(5):
            monitor.observe_action(f"action_{i}")

        all_observations = monitor.get_recent_observations(count=100)

        # Should only keep last 3 observations
        assert len(all_observations) <= 3, "Expected len(all_observations) <= 3"
        # Should have most recent observations
        assert all_observations[0].data["action"] == "action_4", "Expected all_observations[0].data['a... to equal 'action_4'"


class TestAgentMonitorIntegration:
    """Test integration scenarios."""

    def test_incremental_detection_on_observe(self):
        """Test that observations trigger incremental detection."""
        monitor = AgentMonitor()

        # This should trigger loop detection internally
        for _ in range(4):
            monitor.observe_action("repeated_action")

        # The monitor should have detected the loop
        loop_detection = monitor.detect_loop()
        assert loop_detection is not None, "Expected loop_detection is not None"
        assert loop_detection.detected is True, "Expected loop_detection.detected is True"

    def test_multiple_detection_types_simultaneously(self):
        """Test handling multiple detection types at once."""
        monitor = AgentMonitor()

        # Create conditions for multiple detections
        # Loop condition
        for _ in range(4):
            monitor.observe_action("repeated_action")

        # Thrashing condition
        for _ in range(4):
            monitor.observe_action("modify_file", {"file": "thrashed.py"})

        health = monitor.get_health(
            original_task="different task entirely",  # High drift
            tokens_used=9900,  # High context pressure
            token_budget=10000
        )

        assert health.status == HealthStatus.CRITICAL, "Expected health.status to equal HealthStatus.CRITICAL"
        assert len(health.issues) > 1, "Expected len(health.issues) > 1"# Multiple issues detected
